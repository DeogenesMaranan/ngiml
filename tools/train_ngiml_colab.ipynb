{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c69775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/DeogenesMaranan/ngiml\"  # update to your fork if needed\n",
    "REPO_DIR = Path(\"/content/ngiml\")\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
    "else:\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "print(\"Repo ready at\", REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62608c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "DATASET_REPO = \"juhenes/ngiml\"\n",
    "DATASET_REVISION = \"main\"\n",
    "DATA_DIR = \"/content/data\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "snapshot_download(\n",
    "    repo_id=DATASET_REPO,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=DATA_DIR,\n",
    "    revision=DATASET_REVISION,\n",
    "    token=HF_TOKEN,\n",
    "    resume_download=True,\n",
    ")\n",
    "\n",
    "root = Path(DATA_DIR)\n",
    "manifest_files = sorted(\n",
    "    p for p in root.rglob(\"manifest.*\")\n",
    "    if p.name in {\"manifest.parquet\", \"manifest.json\"}\n",
    ")\n",
    "tar_count = sum(1 for _ in root.rglob(\"*.tar\")) + sum(1 for _ in root.rglob(\"*.tar.gz\")) + sum(1 for _ in root.rglob(\"*.tgz\"))\n",
    "\n",
    "print(\"Dataset ready at\", DATA_DIR)\n",
    "print(\"Found manifests:\", [str(p) for p in manifest_files[:5]])\n",
    "print(\"Tar shards count:\", tar_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b05d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive to store checkpoints/logs\n",
    "DRIVE_MOUNT = \"/content/drive\"\n",
    "OUTPUT_DIR = f\"{DRIVE_MOUNT}/MyDrive/ngiml_runs\"\n",
    "\n",
    "drive.mount(DRIVE_MOUNT)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoints will be written to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac42f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "from src.data.dataloaders import AugmentationConfig, load_manifest\n",
    "from src.model.hybrid_ngiml import HybridNGIMLConfig, HybridNGIMLOptimizerConfig, OptimizerGroupConfig\n",
    "from src.model.feature_fusion import FeatureFusionConfig\n",
    "from src.model.unet_decoder import UNetDecoderConfig\n",
    "from src.model.backbones.efficientnet_backbone import EfficientNetBackboneConfig\n",
    "from src.model.backbones.swin_backbone import SwinBackboneConfig\n",
    "from src.model.backbones.residual_noise_branch import ResidualNoiseConfig\n",
    "from src.model.losses import MultiStageLossConfig\n",
    "\n",
    "data_root = Path(DATA_DIR)\n",
    "manifest_names = (\"manifest.parquet\", \"manifest.json\")\n",
    "resolved_manifest_path = data_root / \"manifest_resolved.json\"\n",
    "\n",
    "manifest_candidates = [\n",
    "    resolved_manifest_path,\n",
    "    data_root / \"manifest.parquet\",\n",
    "    data_root / \"manifest.json\",\n",
    "    data_root / \"prepared\" / \"manifest.parquet\",\n",
    "    data_root / \"prepared\" / \"manifest.json\",\n",
    "    data_root / \"ngiml\" / \"manifest.parquet\",\n",
    "    data_root / \"ngiml\" / \"manifest.json\",\n",
    "]\n",
    "MANIFEST_PATH = next((p for p in manifest_candidates if p.exists()), None)\n",
    "\n",
    "if MANIFEST_PATH is None:\n",
    "    discovered = sorted(\n",
    "        p for p in data_root.rglob(\"manifest.*\")\n",
    "        if p.name in manifest_names or p.name == \"manifest_resolved.json\"\n",
    "    )\n",
    "    if discovered:\n",
    "        MANIFEST_PATH = discovered[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No manifest.parquet or manifest.json found under {data_root}. \"\n",
    "            \"Check Cell 2 download path/repo, or set DATA_DIR to the folder containing the manifest file.\"\n",
    "        )\n",
    "\n",
    "# Fast path: if a resolved manifest already exists, reuse it and skip expensive rewrite/index work.\n",
    "if resolved_manifest_path.exists() and resolved_manifest_path.stat().st_size > 0:\n",
    "    MANIFEST_PATH = resolved_manifest_path\n",
    "    print(f\"Using cached resolved manifest: {MANIFEST_PATH}\")\n",
    "else:\n",
    "    print(\"Using manifest:\", MANIFEST_PATH)\n",
    "\n",
    "    def _norm(value: str) -> str:\n",
    "        return str(value).replace(\"\\\\\", \"/\")\n",
    "\n",
    "    def _suffix_score(a_parts, b_parts):\n",
    "        score = 0\n",
    "        for ax, bx in zip(reversed(a_parts), reversed(b_parts)):\n",
    "            if ax != bx:\n",
    "                break\n",
    "            score += 1\n",
    "        return score\n",
    "\n",
    "    # Build a one-time tar index (fast lookup by basename)\n",
    "    tar_files = []\n",
    "    for pat in (\"*.tar\", \"*.tar.gz\", \"*.tgz\"):\n",
    "        tar_files.extend(data_root.rglob(pat))\n",
    "    tar_by_name = {}\n",
    "    for t in tar_files:\n",
    "        tar_by_name.setdefault(t.name, []).append(t)\n",
    "    print(f\"Indexed tar files under {data_root}: {len(tar_files)}\")\n",
    "\n",
    "    def _candidate_paths(value: str, manifest_path: Path, data_root: Path):\n",
    "        normalized = _norm(value)\n",
    "        p = Path(normalized)\n",
    "\n",
    "        candidates = []\n",
    "        if p.is_absolute():\n",
    "            candidates.append(p)\n",
    "        else:\n",
    "            candidates.extend([\n",
    "                manifest_path.parent / p,\n",
    "                data_root / p,\n",
    "                data_root / \"ngiml\" / p,\n",
    "                Path(\"/content\") / p,\n",
    "                Path(\"/content/data\") / p,\n",
    "                Path(\"/content/ngiml\") / p,\n",
    "            ])\n",
    "\n",
    "        if \"prepared/\" in normalized:\n",
    "            suffix = normalized.split(\"prepared/\", 1)[1]\n",
    "            candidates.extend([\n",
    "                data_root / \"prepared\" / suffix,\n",
    "                data_root / \"ngiml\" / \"prepared\" / suffix,\n",
    "                Path(\"/content\") / \"prepared\" / suffix,\n",
    "                Path(\"/content/ngiml\") / \"prepared\" / suffix,\n",
    "            ])\n",
    "\n",
    "        if \"datasets/\" in normalized:\n",
    "            suffix = normalized.split(\"datasets/\", 1)[1]\n",
    "            candidates.extend([\n",
    "                data_root / \"datasets\" / suffix,\n",
    "                data_root / \"ngiml\" / \"datasets\" / suffix,\n",
    "                Path(\"/content\") / \"datasets\" / suffix,\n",
    "                Path(\"/content/ngiml\") / \"datasets\" / suffix,\n",
    "            ])\n",
    "\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        for cand in candidates:\n",
    "            key = cand.as_posix()\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(cand)\n",
    "        return unique\n",
    "\n",
    "    def _match_tar_by_basename(value: str):\n",
    "        name = Path(_norm(value)).name\n",
    "        matches = tar_by_name.get(name, [])\n",
    "        if not matches:\n",
    "            return None\n",
    "        hint_parts = Path(_norm(value)).parts\n",
    "        return max(matches, key=lambda p: _suffix_score(p.parts, hint_parts))\n",
    "\n",
    "    def _resolve_file(value: str, manifest_path: Path, data_root: Path) -> Path:\n",
    "        candidates = _candidate_paths(value, manifest_path, data_root)\n",
    "        for cand in candidates:\n",
    "            if cand.exists():\n",
    "                return cand\n",
    "\n",
    "        if str(value).endswith((\".tar\", \".tar.gz\", \".tgz\")):\n",
    "            tar_match = _match_tar_by_basename(value)\n",
    "            if tar_match is not None:\n",
    "                return tar_match\n",
    "\n",
    "        return candidates[0] if candidates else Path(_norm(value))\n",
    "\n",
    "    def _resolve_path(path_str, manifest_path: Path, data_root: Path) -> str | None:\n",
    "        if path_str is None:\n",
    "            return None\n",
    "        normalized = _norm(path_str)\n",
    "        if \"::\" in normalized:\n",
    "            archive, member = normalized.split(\"::\", 1)\n",
    "            archive_path = _resolve_file(archive, manifest_path, data_root).as_posix()\n",
    "            member_path = _norm(member)\n",
    "            return f\"{archive_path}::{member_path}\"\n",
    "        return _resolve_file(normalized, manifest_path, data_root).as_posix()\n",
    "\n",
    "    def _sample_files_exist(sample) -> bool:\n",
    "        image_path = str(sample.image_path)\n",
    "        if \"::\" in image_path:\n",
    "            archive_path, _ = image_path.split(\"::\", 1)\n",
    "            if not Path(archive_path).exists():\n",
    "                return False\n",
    "        else:\n",
    "            if not Path(image_path).exists():\n",
    "                return False\n",
    "\n",
    "        if sample.mask_path is not None and not Path(sample.mask_path).exists():\n",
    "            return False\n",
    "        if sample.high_pass_path is not None and not Path(sample.high_pass_path).exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    manifest_obj = load_manifest(MANIFEST_PATH)\n",
    "    rewritten = 0\n",
    "    for sample in manifest_obj.samples:\n",
    "        image_new = _resolve_path(sample.image_path, MANIFEST_PATH, data_root)\n",
    "        mask_new = _resolve_path(sample.mask_path, MANIFEST_PATH, data_root) if sample.mask_path else None\n",
    "        hp_new = _resolve_path(sample.high_pass_path, MANIFEST_PATH, data_root) if sample.high_pass_path else None\n",
    "\n",
    "        if image_new != sample.image_path:\n",
    "            sample.image_path = image_new\n",
    "            rewritten += 1\n",
    "        if mask_new != sample.mask_path:\n",
    "            sample.mask_path = mask_new\n",
    "            rewritten += 1\n",
    "        if hp_new != sample.high_pass_path:\n",
    "            sample.high_pass_path = hp_new\n",
    "            rewritten += 1\n",
    "\n",
    "    original_count = len(manifest_obj.samples)\n",
    "    manifest_obj.samples = [s for s in manifest_obj.samples if _sample_files_exist(s)]\n",
    "    filtered_out = original_count - len(manifest_obj.samples)\n",
    "\n",
    "    if not manifest_obj.samples:\n",
    "        raise FileNotFoundError(\n",
    "            \"No valid samples remain after path resolution. \"\n",
    "            f\"Indexed tar files: {len(tar_files)} under {data_root}. \"\n",
    "            \"Likely the downloaded dataset does not contain prepared shards referenced by the manifest.\"\n",
    "        )\n",
    "\n",
    "    with open(resolved_manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest_obj.to_dict(), f)\n",
    "    MANIFEST_PATH = resolved_manifest_path\n",
    "    print(\n",
    "        f\"Wrote resolved manifest to {MANIFEST_PATH} \"\n",
    "        f\"(updated fields: {rewritten}, removed missing samples: {filtered_out})\"\n",
    "    )\n",
    "\n",
    "model_cfg = HybridNGIMLConfig(\n",
    "    efficientnet=EfficientNetBackboneConfig(pretrained=True),\n",
    "    swin=SwinBackboneConfig(model_name=\"swin_tiny_patch4_window7_224\", pretrained=True),\n",
    "    residual=ResidualNoiseConfig(num_kernels=3, base_channels=32, num_stages=4),\n",
    "    fusion=FeatureFusionConfig(fusion_channels=(128, 192, 256, 320)),\n",
    "    decoder=UNetDecoderConfig(decoder_channels=None, out_channels=1, per_stage_heads=True),\n",
    "    optimizer=HybridNGIMLOptimizerConfig(\n",
    "        efficientnet=OptimizerGroupConfig(lr=5e-5, weight_decay=1e-3),\n",
    "        swin=OptimizerGroupConfig(lr=2e-5, weight_decay=1e-2),\n",
    "        residual=OptimizerGroupConfig(lr=1e-4, weight_decay=0.0),\n",
    "        fusion=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        decoder=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "    ),\n",
    "    use_low_level=True,\n",
    "    use_context=True,\n",
    "    use_residual=True,\n",
    ")\n",
    "\n",
    "loss_cfg = MultiStageLossConfig(\n",
    "    dice_weight=1.0,\n",
    "    bce_weight=1.0,\n",
    "    pos_weight=2.0,\n",
    "    stage_weights=None,\n",
    "    smooth=1e-6,\n",
    ")\n",
    "\n",
    "default_aug = AugmentationConfig(\n",
    "    enable=True,\n",
    "    views_per_sample=1,\n",
    "    enable_flips=True,\n",
    "    enable_rotations=True,\n",
    "    max_rotation_degrees=5.0,\n",
    "    enable_random_crop=True,\n",
    "    crop_scale_range=(0.9, 1.0),\n",
    "    enable_color_jitter=True,\n",
    "    color_jitter_factors=(0.9, 1.1),\n",
    "    enable_noise=True,\n",
    "    noise_std_range=(0.0, 0.02),\n",
    ")\n",
    "\n",
    "per_dataset_aug = {\n",
    "    \"IMD2020\": AugmentationConfig(\n",
    "        enable=True,\n",
    "        views_per_sample=2,\n",
    "        enable_flips=True,\n",
    "        enable_rotations=True,\n",
    "        max_rotation_degrees=8.0,\n",
    "        enable_random_crop=True,\n",
    "        crop_scale_range=(0.85, 1.0),\n",
    "        enable_color_jitter=True,\n",
    "        color_jitter_factors=(0.85, 1.15),\n",
    "        enable_noise=True,\n",
    "        noise_std_range=(0.0, 0.03),\n",
    "    ),\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    \"manifest\": str(MANIFEST_PATH),\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 50,\n",
    "    \"num_workers\": 0,\n",
    "    \"amp\": True,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"val_every\": 1,\n",
    "    \"checkpoint_every\": 1,\n",
    "    \"resume\": None,\n",
    "    \"auto_resume\": True,\n",
    "    \"round_robin_seed\": 42,\n",
    "    \"prefetch_factor\": 2,\n",
    "    \"persistent_workers\": False,\n",
    "    \"drop_last\": True,\n",
    "    \"views_per_sample\": 1,\n",
    "    \"max_rotation_degrees\": 5.0,\n",
    "    \"noise_std_max\": 0.02,\n",
    "    \"disable_aug\": False,\n",
    "    \"device\": \"cuda\",\n",
    "    \"aug_seed\": 42,\n",
    "    \"seed\": 42,\n",
    "    \"early_stopping_patience\": 8,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"default_aug\": default_aug,\n",
    "    \"per_dataset_aug\": per_dataset_aug,\n",
    "    \"model_config\": model_cfg,\n",
    "    \"loss_config\": loss_cfg,\n",
    "}\n",
    "\n",
    "print(json.dumps(training_config, indent=2, default=lambda o: dataclasses.asdict(o) if dataclasses.is_dataclass(o) else str(o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71733a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "# Throughput-oriented defaults (safer for Colab than compile+reduce-overhead).\n",
    "recommended_workers = max(2, min(8, (os.cpu_count() or 4) // 2))\n",
    "\n",
    "training_config.update({\n",
    "    \"num_workers\": recommended_workers,\n",
    "    \"persistent_workers\": True,\n",
    "    \"pin_memory\": True,\n",
    "    \"auto_local_cache\": True,\n",
    "    \"local_cache_dir\": \"/content/cache\",\n",
    "    \"compile_model\": False,\n",
    "    \"compile_mode\": \"default\",\n",
    "    \"channels_last\": True,\n",
    "    \"use_tf32\": True,\n",
    "    \"balance_sampling\": True,\n",
    "})\n",
    "\n",
    "# Quality regression guardrails: restore legacy regularization defaults.\n",
    "model_cfg = training_config.get(\"model_config\")\n",
    "if model_cfg is not None and getattr(model_cfg, \"optimizer\", None) is not None:\n",
    "    model_cfg.optimizer.efficientnet.weight_decay = 1e-5\n",
    "    model_cfg.optimizer.swin.weight_decay = 1e-5\n",
    "    model_cfg.optimizer.fusion.weight_decay = 1e-5\n",
    "    model_cfg.optimizer.decoder.weight_decay = 1e-5\n",
    "\n",
    "# IMD2020 views=2 can change optimization dynamics and slow training; keep 1 for parity/stability.\n",
    "if \"per_dataset_aug\" in training_config and \"IMD2020\" in training_config[\"per_dataset_aug\"]:\n",
    "    training_config[\"per_dataset_aug\"][\"IMD2020\"].views_per_sample = 1\n",
    "\n",
    "effective_view_multiplier = {\n",
    "    name: cfg.views_per_sample if cfg.enable else 1\n",
    "    for name, cfg in training_config.get(\"per_dataset_aug\", {}).items()\n",
    "}\n",
    "\n",
    "print(\"Applied speed+quality settings:\")\n",
    "print({k: training_config[k] for k in [\n",
    "    \"num_workers\",\n",
    "    \"persistent_workers\",\n",
    "    \"pin_memory\",\n",
    "    \"auto_local_cache\",\n",
    "    \"local_cache_dir\",\n",
    "    \"compile_model\",\n",
    "    \"compile_mode\",\n",
    "    \"channels_last\",\n",
    "    \"use_tf32\",\n",
    "    \"balance_sampling\",\n",
    "]})\n",
    "print(\"Per-dataset views_per_sample:\", effective_view_multiplier)\n",
    "\n",
    "# Effective config to be passed to TrainConfig\n",
    "print(\"Effective training config (post-settings):\")\n",
    "print(json.dumps(training_config, indent=2, default=lambda o: dataclasses.asdict(o) if dataclasses.is_dataclass(o) else str(o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9960608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from tools import train_ngiml\n",
    "\n",
    "# Ensure latest module state in this kernel\n",
    "reload(train_ngiml)\n",
    "\n",
    "cfg = train_ngiml.TrainConfig(**training_config)\n",
    "train_ngiml.run_training(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03601a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
