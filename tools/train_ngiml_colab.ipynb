{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/your-user/ngiml.git\"  # update to your fork if needed\n",
    "REPO_DIR = Path(\"/content/ngiml\")\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
    "else:\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "print(\"Repo ready at\", REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62608c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")  # optional if repo is private\n",
    "DATASET_REPO = \"your-hf-user/your-prepared-dataset\"  # update to your dataset repo id\n",
    "DATASET_REVISION = \"main\"\n",
    "DATA_DIR = \"/content/data\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "snapshot_download(\n",
    "    repo_id=DATASET_REPO,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=DATA_DIR,\n",
    "    revision=DATASET_REVISION,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "print(\"Dataset ready at\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive to store checkpoints/logs\n",
    "DRIVE_MOUNT = \"/content/drive\"\n",
    "OUTPUT_DIR = f\"{DRIVE_MOUNT}/MyDrive/ngiml_runs\"\n",
    "\n",
    "drive.mount(DRIVE_MOUNT)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoints will be written to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac42f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "from src.data.dataloaders import AugmentationConfig\n",
    "from src.model.hybrid_ngiml import HybridNGIMLConfig, HybridNGIMLOptimizerConfig, OptimizerGroupConfig\n",
    "from src.model.feature_fusion import FeatureFusionConfig\n",
    "from src.model.unet_decoder import UNetDecoderConfig\n",
    "from src.model.backbones.efficientnet_backbone import EfficientNetBackboneConfig\n",
    "from src.model.backbones.swin_backbone import SwinBackboneConfig\n",
    "from src.model.backbones.residual_noise_branch import ResidualNoiseConfig\n",
    "from src.model.losses import MultiStageLossConfig\n",
    "\n",
    "MANIFEST_PATH = Path(DATA_DIR) / \"manifest.json\"\n",
    "\n",
    "model_cfg = HybridNGIMLConfig(\n",
    "    efficientnet=EfficientNetBackboneConfig(pretrained=True),\n",
    "    swin=SwinBackboneConfig(model_name=\"swin_tiny_patch4_window7_224\", pretrained=True),\n",
    "    residual=ResidualNoiseConfig(gaussian_kernel=5, gaussian_sigma=1.2, highpass_strength=1.0),\n",
    "    fusion=FeatureFusionConfig(fusion_channels=(128, 192, 256, 320)),\n",
    "    decoder=UNetDecoderConfig(decoder_channels=None, out_channels=1, per_stage_heads=True),\n",
    "    optimizer=HybridNGIMLOptimizerConfig(\n",
    "        efficientnet=OptimizerGroupConfig(lr=5e-5, weight_decay=1e-5),\n",
    "        swin=OptimizerGroupConfig(lr=2e-5, weight_decay=1e-5),\n",
    "        residual=OptimizerGroupConfig(lr=1e-4, weight_decay=0.0),\n",
    "        fusion=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        decoder=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "    ),\n",
    "    use_low_level=True,\n",
    "    use_context=True,\n",
    "    use_residual=True,\n",
    ")\n",
    "\n",
    "loss_cfg = MultiStageLossConfig(\n",
    "    dice_weight=1.0,\n",
    "    bce_weight=1.0,\n",
    "    pos_weight=2.0,\n",
    "    stage_weights=None,\n",
    "    smooth=1e-6,\n",
    ")\n",
    "\n",
    "default_aug = AugmentationConfig(\n",
    "    enable=True,\n",
    "    copies_per_sample=1,\n",
    "    enable_flips=True,\n",
    "    enable_rotations=True,\n",
    "    max_rotation_degrees=5.0,\n",
    "    enable_random_crop=True,\n",
    "    crop_scale_range=(0.9, 1.0),\n",
    "    enable_color_jitter=True,\n",
    "    color_jitter_factors=(0.9, 1.1),\n",
    "    enable_noise=True,\n",
    "    noise_std_range=(0.0, 0.02),\n",
    ")\n",
    "\n",
    "per_dataset_aug = {\n",
    "    # \"dataset_name_from_manifest\": AugmentationConfig(enable=False),\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    \"manifest\": str(MANIFEST_PATH),\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 10,\n",
    "    \"num_workers\": 2,\n",
    "    \"amp\": True,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"val_every\": 1,\n",
    "    \"checkpoint_every\": 1,\n",
    "    \"resume\": None,\n",
    "    \"round_robin_seed\": 0,\n",
    "    \"prefetch_factor\": 2,\n",
    "    \"persistent_workers\": False,\n",
    "    \"drop_last\": True,\n",
    "    \"copies_per_sample\": 1,\n",
    "    \"max_rotation_degrees\": 5.0,\n",
    "    \"noise_std_max\": 0.02,\n",
    "    \"disable_aug\": False,\n",
    "    \"device\": \"cuda\",\n",
    "    \"aug_seed\": None,\n",
    "    \"default_aug\": default_aug,\n",
    "    \"per_dataset_aug\": per_dataset_aug,\n",
    "    \"model_config\": model_cfg,\n",
    "    \"loss_config\": loss_cfg,\n",
    "}\n",
    "\n",
    "print(json.dumps(training_config, indent=2, default=lambda o: dataclasses.asdict(o) if dataclasses.is_dataclass(o) else str(o)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9960608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import train_ngiml\n",
    "cfg = train_ngiml.TrainConfig(**training_config)\n",
    "train_ngiml.run_training(cfg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
