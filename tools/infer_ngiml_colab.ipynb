{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76104c7b",
   "metadata": {},
   "source": [
    "# NGIML Inference (Colab)\n",
    "Load a trained checkpoint and run a sanity-check inference on one sample from the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/DeogenesMaranan/ngiml\"\n",
    "REPO_DIR = Path(\"/content/ngiml\")\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
    "else:\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "print(\"Repo ready at\", REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f890fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_MOUNT = \"/content/drive\"\n",
    "RUNS_DIR = Path(f\"{DRIVE_MOUNT}/MyDrive/ngiml_runs\")\n",
    "DATA_DIR = Path(\"/content/data\")\n",
    "\n",
    "drive.mount(DRIVE_MOUNT)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Runs dir:\", RUNS_DIR)\n",
    "print(\"Data dir:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b13d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.dataloaders import load_manifest, _load_from_tar_npz, _load_from_npz, _load_image\n",
    "from src.model.hybrid_ngiml import HybridNGIML, HybridNGIMLConfig, HybridNGIMLOptimizerConfig, OptimizerGroupConfig\n",
    "from src.model.feature_fusion import FeatureFusionConfig\n",
    "from src.model.unet_decoder import UNetDecoderConfig\n",
    "from src.model.backbones.efficientnet_backbone import EfficientNetBackboneConfig\n",
    "from src.model.backbones.swin_backbone import SwinBackboneConfig\n",
    "from src.model.backbones.residual_noise_branch import ResidualNoiseConfig\n",
    "\n",
    "# Prefer best checkpoint; fallback to latest epoch checkpoint.\n",
    "best_ckpt_candidates = sorted(RUNS_DIR.rglob(\"checkpoints/best_checkpoint.pt\"))\n",
    "if best_ckpt_candidates:\n",
    "    CKPT_PATH = best_ckpt_candidates[-1]\n",
    "else:\n",
    "    ckpt_candidates = sorted(RUNS_DIR.rglob(\"checkpoints/checkpoint_epoch_*.pt\"))\n",
    "    if not ckpt_candidates:\n",
    "        raise FileNotFoundError(f\"No checkpoint found under {RUNS_DIR}/**/checkpoints/\")\n",
    "    CKPT_PATH = ckpt_candidates[-1]\n",
    "print(\"Using checkpoint:\", CKPT_PATH)\n",
    "\n",
    "def _find_manifest(data_dir: Path) -> Path | None:\n",
    "    manifest_candidates = [\n",
    "        data_dir / \"manifest_resolved.json\",\n",
    "        data_dir / \"manifest.parquet\",\n",
    "        data_dir / \"manifest.json\",\n",
    "        data_dir / \"prepared\" / \"manifest.parquet\",\n",
    "        data_dir / \"prepared\" / \"manifest.json\",\n",
    "        data_dir / \"ngiml\" / \"manifest.parquet\",\n",
    "        data_dir / \"ngiml\" / \"manifest.json\",\n",
    "    ]\n",
    "    manifest_candidates += sorted(data_dir.rglob(\"manifest.parquet\"))\n",
    "    manifest_candidates += sorted(data_dir.rglob(\"manifest.json\"))\n",
    "    return next((p for p in manifest_candidates if p.exists()), None)\n",
    "\n",
    "# Find manifest, auto-download dataset metadata if missing\n",
    "MANIFEST_PATH = _find_manifest(DATA_DIR)\n",
    "if MANIFEST_PATH is None:\n",
    "    print(f\"No manifest found under {DATA_DIR}. Attempting dataset download...\")\n",
    "    from huggingface_hub import login, snapshot_download\n",
    "\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "    DATASET_REPO = \"juhenes/ngiml\"\n",
    "    DATASET_REVISION = \"main\"\n",
    "    if HF_TOKEN:\n",
    "        login(token=HF_TOKEN)\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=DATASET_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=str(DATA_DIR),\n",
    "        revision=DATASET_REVISION,\n",
    "        token=HF_TOKEN or None,\n",
    "        resume_download=True,\n",
    "    )\n",
    "    MANIFEST_PATH = _find_manifest(DATA_DIR)\n",
    "\n",
    "if MANIFEST_PATH is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No manifest found under {DATA_DIR} even after download. \"\n",
    "        \"Set DATA_DIR to your dataset root or provide HF_TOKEN env var if dataset is private.\"\n",
    "    )\n",
    "print(\"Using manifest:\", MANIFEST_PATH)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model_cfg = HybridNGIMLConfig(\n",
    "    efficientnet=EfficientNetBackboneConfig(pretrained=True),\n",
    "    swin=SwinBackboneConfig(model_name=\"swin_tiny_patch4_window7_224\", pretrained=True),\n",
    "    residual=ResidualNoiseConfig(num_kernels=3, base_channels=32, num_stages=4),\n",
    "    fusion=FeatureFusionConfig(fusion_channels=(128, 192, 256, 320)),\n",
    "    decoder=UNetDecoderConfig(decoder_channels=None, out_channels=1, per_stage_heads=True),\n",
    "    optimizer=HybridNGIMLOptimizerConfig(\n",
    "        efficientnet=OptimizerGroupConfig(lr=5e-5, weight_decay=1e-5),\n",
    "        swin=OptimizerGroupConfig(lr=2e-5, weight_decay=1e-5),\n",
    "        residual=OptimizerGroupConfig(lr=1e-4, weight_decay=0.0),\n",
    "        fusion=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        decoder=OptimizerGroupConfig(lr=1e-4, weight_decay=1e-5),\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "    ),\n",
    "    use_low_level=True,\n",
    "    use_context=True,\n",
    "    use_residual=True,\n",
    " )\n",
    "\n",
    "model = HybridNGIML(model_cfg).to(device)\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "print(\"Checkpoint epoch:\", ckpt.get(\"epoch\"))\n",
    "print(\"Missing keys:\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n",
    "if missing:\n",
    "    print(\"First missing:\", missing[:5])\n",
    "if unexpected:\n",
    "    print(\"First unexpected:\", unexpected[:5])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad229354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "manifest = load_manifest(MANIFEST_PATH)\n",
    "test_samples = [s for s in manifest.samples if s.split == \"test\"]\n",
    "if not test_samples:\n",
    "    raise RuntimeError(\"No samples found in test split\")\n",
    "\n",
    "# Keep only fake/manipulated test samples (label==1; fallback to mask presence)\n",
    "fake_test_samples = [s for s in test_samples if getattr(s, \"label\", 0) == 1 or s.mask_path is not None]\n",
    "if not fake_test_samples:\n",
    "    raise RuntimeError(\"No fake samples found in test split\")\n",
    "\n",
    "samples_to_show = fake_test_samples[:10]\n",
    "print(f\"Showing {len(samples_to_show)} fake samples from test split\")\n",
    "\n",
    "# Build a one-time tar index for robust path remapping\n",
    "tar_files = []\n",
    "for pat in (\"*.tar\", \"*.tar.gz\", \"*.tgz\"):\n",
    "    tar_files.extend(DATA_DIR.rglob(pat))\n",
    "tar_by_name = {}\n",
    "for t in tar_files:\n",
    "    tar_by_name.setdefault(t.name, []).append(t)\n",
    "\n",
    "def _norm(value: str) -> str:\n",
    "    return str(value).replace('\\\\', '/')\n",
    "\n",
    "def _suffix_score(a_parts, b_parts):\n",
    "    score = 0\n",
    "    for ax, bx in zip(reversed(a_parts), reversed(b_parts)):\n",
    "        if ax != bx:\n",
    "            break\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "def _candidate_paths(value: str):\n",
    "    normalized = _norm(value)\n",
    "    p = Path(normalized)\n",
    "    manifest_parent = Path(MANIFEST_PATH).parent\n",
    "    candidates = []\n",
    "\n",
    "    if p.is_absolute():\n",
    "        candidates.append(p)\n",
    "    else:\n",
    "        candidates.extend([\n",
    "            manifest_parent / p,\n",
    "            DATA_DIR / p,\n",
    "            DATA_DIR / \"ngiml\" / p,\n",
    "            Path(\"/content\") / p,\n",
    "            Path(\"/content/data\") / p,\n",
    "            Path(\"/content/ngiml\") / p,\n",
    "        ])\n",
    "\n",
    "    if \"prepared/\" in normalized:\n",
    "        suffix = normalized.split(\"prepared/\", 1)[1]\n",
    "        candidates.extend([\n",
    "            DATA_DIR / \"prepared\" / suffix,\n",
    "            DATA_DIR / \"ngiml\" / \"prepared\" / suffix,\n",
    "            Path(\"/content\") / \"prepared\" / suffix,\n",
    "            Path(\"/content/ngiml\") / \"prepared\" / suffix,\n",
    "        ])\n",
    "\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for cand in candidates:\n",
    "        key = cand.as_posix()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(cand)\n",
    "    return unique\n",
    "\n",
    "def _match_tar_by_basename(value: str):\n",
    "    name = Path(_norm(value)).name\n",
    "    matches = tar_by_name.get(name, [])\n",
    "    if not matches:\n",
    "        return None\n",
    "    hint_parts = Path(_norm(value)).parts\n",
    "    return max(matches, key=lambda p: _suffix_score(p.parts, hint_parts))\n",
    "\n",
    "def _resolve_file(value: str) -> Path:\n",
    "    candidates = _candidate_paths(value)\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "\n",
    "    if str(value).endswith((\".tar\", \".tar.gz\", \".tgz\")):\n",
    "        tar_match = _match_tar_by_basename(value)\n",
    "        if tar_match is not None:\n",
    "            return tar_match\n",
    "\n",
    "    return candidates[0] if candidates else Path(_norm(value))\n",
    "\n",
    "def _resolve_path(path_str):\n",
    "    if path_str is None:\n",
    "        return None\n",
    "    normalized = _norm(path_str)\n",
    "    if \"::\" in normalized:\n",
    "        archive, member = normalized.split(\"::\", 1)\n",
    "        archive_path = _resolve_file(archive).as_posix()\n",
    "        member_path = _norm(member)\n",
    "        return f\"{archive_path}::{member_path}\"\n",
    "    return _resolve_file(normalized).as_posix()\n",
    "\n",
    "def _load_image_and_mask(record):\n",
    "    image_path = _resolve_path(record.image_path)\n",
    "    if '::' in image_path and image_path.endswith('.npz'):\n",
    "        image, mask, _ = _load_from_tar_npz(image_path)\n",
    "    elif image_path.endswith('.npz'):\n",
    "        image, mask, _ = _load_from_npz(image_path)\n",
    "    else:\n",
    "        image = _load_image(image_path)\n",
    "        mask = None\n",
    "        if record.mask_path is not None:\n",
    "            mask_path = _resolve_path(record.mask_path)\n",
    "            mask = _load_image(mask_path)\n",
    "\n",
    "    image = image.float()\n",
    "    if image.max() > 1.0:\n",
    "        image = image / 255.0\n",
    "\n",
    "    if mask is None:\n",
    "        mask = torch.zeros((1, image.shape[-2], image.shape[-1]), dtype=torch.float32)\n",
    "    else:\n",
    "        mask = mask.float()\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        if mask.shape[0] > 1:\n",
    "            mask = mask[:1]\n",
    "        if mask.max() > 1.0:\n",
    "            mask = mask / 255.0\n",
    "        if tuple(mask.shape[-2:]) != tuple(image.shape[-2:]):\n",
    "            mask = F.interpolate(mask.unsqueeze(0), size=image.shape[-2:], mode='nearest').squeeze(0)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "rows = len(samples_to_show)\n",
    "fig, axes = plt.subplots(rows, 4, figsize=(16, 3 * rows))\n",
    "if rows == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "col_titles = [\"Photo\", \"Ground Truth\", \"Prediction\", \"Overlay (Prediction on Photo)\"]\n",
    "for col, title in enumerate(col_titles):\n",
    "    axes[0, col].set_title(title)\n",
    "\n",
    "for i, sample in enumerate(samples_to_show):\n",
    "    image, gt_mask = _load_image_and_mask(sample)\n",
    "\n",
    "    x = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, target_size=image.shape[-2:])[-1]\n",
    "        pred_prob = torch.sigmoid(logits)[0, 0].detach().cpu()\n",
    "\n",
    "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    gt_np = gt_mask[0].cpu().numpy()\n",
    "    pred_np = pred_prob.numpy()\n",
    "\n",
    "    pred_bin = (pred_np >= 0.5).astype(np.float32)\n",
    "    overlay = img_np.copy()\n",
    "    overlay[..., 0] = np.clip(overlay[..., 0] + 0.5 * pred_bin, 0, 1)\n",
    "    overlay[..., 1] = np.clip(overlay[..., 1] * (1.0 - 0.35 * pred_bin), 0, 1)\n",
    "    overlay[..., 2] = np.clip(overlay[..., 2] * (1.0 - 0.35 * pred_bin), 0, 1)\n",
    "\n",
    "    axes[i, 0].imshow(img_np)\n",
    "    axes[i, 1].imshow(gt_np, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[i, 2].imshow(pred_np, cmap='magma', vmin=0, vmax=1)\n",
    "    axes[i, 3].imshow(overlay)\n",
    "\n",
    "    for j in range(4):\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "    axes[i, 0].set_ylabel(f\"{sample.dataset}\\n#{i+1}\", rotation=0, labelpad=28, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import io\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded.\")\n",
    "\n",
    "filename, file_bytes = next(iter(uploaded.items()))\n",
    "pil_img = Image.open(io.BytesIO(file_bytes)).convert(\"RGB\")\n",
    "img_np = np.array(pil_img).astype(np.float32) / 255.0\n",
    "img = torch.from_numpy(img_np).permute(2, 0, 1)\n",
    "\n",
    "# Ensure spatial size is compatible with Swin/decoder (multiple of 32)\n",
    "h, w = img.shape[-2], img.shape[-1]\n",
    "\n",
    "# --- Start of added code for OOM fix ---\n",
    "MAX_SIDE_LENGTH = 768 # You can adjust this value if still getting OOM\n",
    "if max(h, w) > MAX_SIDE_LENGTH:\n",
    "    scale_factor = MAX_SIDE_LENGTH / max(h, w)\n",
    "    new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "else:\n",
    "    new_h, new_w = h, w\n",
    "\n",
    "target_h = int(math.ceil(new_h / 32) * 32)\n",
    "target_w = int(math.ceil(new_w / 32) * 32)\n",
    "# --- End of added code for OOM fix ---\n",
    "\n",
    "# Original resizing logic, now using target_h, target_w from OOM fix\n",
    "if (target_h, target_w) != (h, w):\n",
    "    img = F.interpolate(img.unsqueeze(0), size=(target_h, target_w), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "x = img.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(x, target_size=img.shape[-2:])[-1]\n",
    "    pred_prob = torch.sigmoid(logits)[0, 0].detach().cpu().numpy()\n",
    "\n",
    "img_show = img.permute(1, 2, 0).cpu().numpy()\n",
    "pred_bin = (pred_prob >= 0.5).astype(np.float32)\n",
    "\n",
    "overlay = img_show.copy()\n",
    "overlay[..., 0] = np.clip(overlay[..., 0] + 0.5 * pred_bin, 0, 1)\n",
    "overlay[..., 1] = np.clip(overlay[..., 1] * (1.0 - 0.35 * pred_bin), 0, 1)\n",
    "overlay[..., 2] = np.clip(overlay[..., 2] * (1.0 - 0.35 * pred_bin), 0, 1)\n",
    "\n",
    "print(f\"Uploaded: {filename}\")\n",
    "print(\"Input tensor shape:\", tuple(x.shape))\n",
    "print(\"Prediction stats min/mean/max:\", float(pred_prob.min()), float(pred_prob.mean()), float(pred_prob.max()))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Uploaded Photo\")\n",
    "plt.imshow(img_show)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(pred_prob, cmap=\"magma\", vmin=0, vmax=1)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(overlay)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44066f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fvcore iopath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity stats (params, FLOPs, MACs)\n",
    "from tools.local_infer_helpers import get_model_complexity_stats\n",
    "\n",
    "def _human(n: float | int | None) -> str:\n",
    "    if n is None:\n",
    "        return \"n/a\"\n",
    "    n = float(n)\n",
    "    if n >= 1e12:\n",
    "        return f\"{n/1e12:.3f}T\"\n",
    "    if n >= 1e9:\n",
    "        return f\"{n/1e9:.3f}B\"\n",
    "    if n >= 1e6:\n",
    "        return f\"{n/1e6:.3f}M\"\n",
    "    if n >= 1e3:\n",
    "        return f\"{n/1e3:.3f}K\"\n",
    "    return f\"{n:.0f}\"\n",
    "\n",
    "# Keep aligned with your training/inference resolution.\n",
    "PROFILE_INPUT_SIZE = (1, 3, 320, 320)\n",
    "stats = get_model_complexity_stats(model, input_size=PROFILE_INPUT_SIZE)\n",
    "\n",
    "print(\"Model complexity:\")\n",
    "print({\n",
    "    \"input_size\": stats[\"input_size\"],\n",
    "    \"total_params\": f\"{stats['total_params']:,} ({_human(stats['total_params'])})\",\n",
    "    \"trainable_params\": f\"{stats['trainable_params']:,} ({_human(stats['trainable_params'])})\",\n",
    "    \"frozen_params\": f\"{stats['frozen_params']:,} ({_human(stats['frozen_params'])})\",\n",
    "    \"flops\": f\"{_human(stats['flops'])}\" if stats.get(\"flops\") is not None else \"n/a\",\n",
    "    \"macs\": f\"{_human(stats['macs'])}\" if stats.get(\"macs\") is not None else \"n/a\",\n",
    "})\n",
    "if stats.get(\"flops_error\"):\n",
    "    print(\"FLOPs note:\", stats[\"flops_error\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
